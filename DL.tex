
%\documentclass[12pt,draft]{article}
\documentclass[12pt]{article}

\usepackage{CJK}
\usepackage{mathrsfs}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{float}
\usepackage[dvips]{graphicx}
\usepackage{subfigure}
\usepackage[font=small]{caption}
\usepackage{threeparttable}
\usepackage{cases}
\usepackage{multicol}
\usepackage{url}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{overpic}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{natbib}
\usepackage{graphicx}

\numberwithin{equation}{section}


\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}

\setlength{\parskip}{0.3\baselineskip}
\setlength{\headheight}{15pt}
\begin{document}\small
  \renewcommand\figurename{Fig.}
  %\renewcommand\arraystretch{1.0}
    \title{Deep Learning\cite{Goodfellow-et-al-2016-Book} Notes}
    \author{Yan JIN}
    \pagestyle{fancy}\fancyhf{}
    \lhead{}\rhead{JIN Yan}
    \lfoot{\textit{}}\cfoot{}\rfoot{\thepage}
    \renewcommand{\headrulewidth}{1.pt}
    \renewcommand{\footrulewidth}{1.pt}
  \maketitle

\section{3.8 Expectation, Variance and Covariance}
\begin{align*}
Expectation: \mathbb{E}_{X \sim P}[f(x)]&=\int p(x)f(x)dx \\
Variance: Var(f(x))&=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2] \\
Covariance: Cov(f(x),g(x))&=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(x)-\mathbb{E}[g(x)])]
\end{align*}
\section{5.4 Estimators, Bias and Variance}
\subsection{5.4.1 Point Estimation}
\subsection{5.4.2 Bias}
\[ bias(\hat{{\boldsymbol\theta}}_m)=\mathbb{E}(\hat{\boldsymbol\theta}_m)-\boldsymbol\theta
\]

Proof of formula (5.39):
\begin{align*}
\hat{\mu}_m&=\mathbb{E}[x^{(i)}]=\frac{1}{m}\sum_{i=1}^{m}x^{(i)} \\
\mu&=\mathbb{E}[\hat{\mu}_m] \\
Var(x^{(i)})&=\mathbb{E}[(x^{(i)}-\mu)^2]=\sigma^2 \\
Var(\hat{\mu}_m)&=\mathbb{E}[(\hat{\mu}_m-\mu)^2] =\frac{\sigma^2}{m}
\end{align*}

so:
\begin{align*}
\mathbb{E}[\hat{\sigma}^2_m]
&=\mathbb{E}[\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\hat{\mu}_m)^2] \\
&=\frac{1}{m}\mathbb{E}[\sum_{i=1}^{m}(x^{(i)}-\mu+\mu-\hat{\mu}_m)^2] \\
&=\frac{1}{m}\mathbb{E}[\sum_{i=1}^{m}(x^{(i)}-\mu)^2+2\sum_{i=1}^{m}(x^{(i)}-\mu)(\mu-\hat{\mu}_m)+\sum_{i=1}^{m}(\mu-\hat{\mu}_m)^2] \\
&=\frac{1}{m}\mathbb{E}[\sum_{i=1}^{m}(x^{(i)}-\mu)^2+2m(\hat{\mu}_m-\mu)(\mu-\hat{\mu}_m)+m(\mu-\hat{\mu}_m)^2] \\
&=\frac{1}{m}\mathbb{E}[\sum_{i=1}^{m}(x^{(i)}-\mu)^2-m(\hat{\mu}_m-\mu)^2] \\
&=\frac{1}{m}(\sum_{i=1}^{m}\mathbb{E}[(x^{(i)}-\mu)^2]-m\mathbb{E}[(\hat{\mu}_m-\mu)^2]) \\
&=\frac{1}{m}(mVar(x^{(i)})-mVar(\hat{\mu}_m)) \\
&=Var(x^{(i)})-Var(\hat{\mu}_m) \\
&=\sigma^2-\frac{\sigma^2}{m}=\frac{m-1}{m}\sigma^2
\end{align*}

\subsection{5.4.2 Variance and Standard Error}
Variance:$ Var(\hat{\theta})$

Standard Error: $SE(\hat{\theta})=\sqrt{Var(\hat{\theta})}$

\subsection{5.4.4 Trading off Bias and Variance to Minimize Mean Square Error}

Proof (5.54):
\begin{align*}
MSE
&=\mathbb{E}[(\hat{\theta}_m-\theta)^2] \\
&=\mathbb{E}[\hat{\theta}_m^2]-2\theta\mathbb{E}(\hat{\theta}_m)+\theta^2 \\
Bias(\hat{\theta}_m)^2
&=(\mathbb{E}[\hat{\theta}_m]-\theta)^2 \\
&=\mathbb{E}[\hat{\theta}_m]^2-2\mathbb{E}[\hat{\theta}_m]\theta+\theta^2 \\
Var(\hat{\theta}_m)
&=\mathbb{E}[(\hat{\theta}_m-\mathbb{E}[\hat{\theta}_m])^2]\\
&=\mathbb{E}[\hat{\theta}_m^2-2\hat{\theta}_m\mathbb{E}[\hat{\theta}_m]+\mathbb{E}[\hat{\theta}_m]^2]\\
&=\mathbb{E}[\hat{\theta}_m^2]-\mathbb{E}[\hat{\theta}_m]^2\\
\Rightarrow
MSE&=Bias(\hat{\theta}_m)^2+Var(\hat{\theta}_m)
\end{align*}

\section{Frequentist Statistics and Baysian Statistics}
Frequentist: Estimate a single value of $\boldsymbol\theta$, then making all predictions 
thereafter based on that \textbf{one} estimate;

Baysian: Consider \textbf{all} possible values of $\boldsymbol\theta$ when making a prediction;

Frequentist: The true parameter value $\boldsymbol\theta$ is \textbf{fixed but unknown}, while $\hat{\boldsymbol\theta}$ is a random variable and a function of \textbf{the dataset}(which is seen as \textbf{random});

Baysian: \textbf{Dataset} is directly observed and is \textbf{not random}; the true parameter value $\boldsymbol\theta$ is \textbf{unknown or uncertain} and thus is represented as a random variable;

Differences between MLE(Maximum Likelihood Estimation) and Bayesian estimation:
\begin{enumerate}
	\item MLE: Make predictions using a \textbf{point estimate} of $\boldsymbol\theta$;
	
	Bayesian: Using a \textbf{full distribution} over $\boldsymbol\theta$;
	\item MLE: Address the uncertainty on a given point estimate of $\boldsymbol\theta$ by evaluating its \textbf{variance};
	
	Bayesian: Simply \textbf{integrate over it};
	\item Baysian: Use a priori, which expresses a preference for \textbf{simpler and smooth models}, and seems as a source of \textbf{subjective human judgment} impacting the predictions;
	\item Baysian: \textbf{Generalize much better} when training data is \textbf{small}, but \textbf{high computation cost} when training data is \textbf{large};
\end{enumerate}

\subsection{Frequentist Statistics - Maximum Likelihood Estimation (MLE)}
For data samples ${x^{(1)},...,x^{(m)}}$ drawn independently from \textbf{the true but unknown} data generating distribution $p_{data}(\boldsymbol{x})$

$p_{model}(\boldsymbol{x;\theta})$ is a parametric family of probability distribution over the space indexed by $\boldsymbol{\theta}$ for estimating the $p_{data}(\boldsymbol{x})$.

\[
\boldsymbol{\theta_{ML}}=\underset{\boldsymbol{\theta}}{argmax}\ p_{model}({\mathbb{X};\boldsymbol{\theta}})=
\]

\subsection{Baysian Statistics}
Prior probability distribution(the prior): $p(\boldsymbol\theta)$

For data samples ${x^{(1)},...,x^{(m)}}$, we reform the belief about $\boldsymbol\theta$(the posterior$p(\boldsymbol\theta|x^{(1)},...,x^{(m)})$) by the data likelihood $p(x^{(1)},...,x^{(m)}|\boldsymbol\theta)$ and the prior $p(\boldsymbol\theta)$ via \textbf{Bayes' rule}: \[
p(\boldsymbol\theta|x^{(1)},...,x^{(m)})=\frac{p(x^{(1)},...,x^{(m)}|\boldsymbol\theta)p(\boldsymbol\theta)}{p(x^{(1)},...,x^{(m)})}
\]

\subsection{Maximum A Posteriori (MAP) Estimation}
\[
\boldsymbol\theta_{MAP}=\underset{\boldsymbol\theta}{argmax} \  p(\boldsymbol{\theta|x})=\underset{\boldsymbol\theta}{argmax} \ log \  p(\boldsymbol{x|\theta})+log\ p(\boldsymbol{\theta})
\]
MAP has the advantage of leveraging information that is brought by \textbf{the prior} and cannot be found in \textbf{the training data}. This information helps to \textbf{reduce the variance} in the MAP point estimate (compare to ML estimate), but \textbf{increase bias}.
\[
MLE(log\ p(\boldsymbol{\theta|x})) \ + \ Regularization\ with\ weight\ decay(log\ p(\boldsymbol{\theta})) = MAP \  to \ Bayesian \ inference.
\]

\section{Chapter 11 Practical Methodology}
Practical design process:
\begin{enumerate}
	\item Determine error metric and target value;
	\item Establish a Baseline Model;
	\item Determine bottlenecks in performance;
	\item Repeatedly make incremental changes: gathering new data, adjusting hyperparameters, or changing algorithms;
\end{enumerate}

\subsection{11.3 Determining Whether to Gather More Data}
\begin{enumerate}
	\item Determine whether the performance on the training set is acceptable; 
	
	If performance on the training set is poor:
	\item Increase the size of the model: add more layers; add more hidden unites to each layer; turning 
	the learning rate etc.
	
	If still not work well: data needed to be cleaned or gathered;
	
	Else:
	\item Measure performance on test set;
	
	If performance good, done!
	
	Else if test set performance is much worse than training set performance:
	\item Gather data;
	
	If not easy to gather data:
	\item Reduce the size of the model; Improve regularization(adjust weight decay coefficients or add dropout);
	
	If test set performance is still unacceptable:
	\item Gather data;
\end{enumerate}



\renewcommand\refname{Reference}
\bibliographystyle{plain}
\bibliography{DL}

  \clearpage
\end{document}